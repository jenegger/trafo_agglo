{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b20ca15-7a84-4658-8359-b9a240339f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from itertools import combinations\n",
    "import torch\n",
    "import math \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%run r3b_clustering_def.ipynb\n",
    "%run agglomerative_def.ipynb\n",
    "%run ff_models_def.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f5e305-7b08-46d0-87ac-090993314e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: check that you do correctly shifting the time info for the agglo model...\n",
    "def subset_data_sel(data,sel_type,model,f_out_name):\n",
    "    if (model == \"r3b\"):\n",
    "        reco_vals = run_r3b_clustering(data,0.25)\n",
    "    elif (model == \"agglo\"):\n",
    "        reco_vals = run_agglo_clustering(data,3540)\n",
    "    else: \n",
    "        print(\"something went wrong, have you chosen the correct model?\")\n",
    "    true_vals = get_true_combinatorics(data)    \n",
    "    print(\"now I have all simple stuff done\")\n",
    "    idx = 0\n",
    "    fmt = '%i','%1.5f', '%1.5f','%1.5f','%1.2f' \n",
    "    arr = np.empty((0, 5), float)\n",
    "    with open(f_out_name, 'w') as f:\n",
    "        for i in range(0,int(np.max(data,axis=0)[0]+1)-(int(np.max(data,axis=0)[0]+1)%3) ,3):\n",
    "        #for i in range(0,int(np.max(data,axis=0)[0])-(int(np.max(data,axis=0)[0])%3) ,3):\n",
    "            if i % 1000 == 0:\n",
    "                percentage = (i /int(np.max(data,axis=0)[0]+1)) * 100\n",
    "                print(f\"Progress: {percentage:.2f}%\")\n",
    "            mask = ((data[:,0] == i) | (data[:,0] == i+1) | (data[:,0] == i+2))\n",
    "            evt = data[mask]\n",
    "            pair_indices = list(combinations(range(evt.shape[0]), 2))\n",
    "            ##pair indices which belong together...\n",
    "            pred_comb = []\n",
    "            ##TJ this is the new part with true stuff\n",
    "            true_comb = []\n",
    "            for m in range(evt.shape[0]):\n",
    "                pred_comb.append((m,m))\n",
    "                true_comb.append((m,m))\n",
    "            for j in range (len(pair_indices)):\n",
    "                if reco_vals[idx+j] > 0.75:\n",
    "                    pred_comb.append(pair_indices[j])\n",
    "                if true_vals[idx+j] > 0.75:   \n",
    "                    true_comb.append(pair_indices[j])\n",
    "            G = nx.Graph()\n",
    "            G.add_edges_from(pred_comb)\n",
    "            connected_components = list(nx.connected_components(G))\n",
    "            ##TJ again, this it the addition for the true values\n",
    "            T = nx.Graph()\n",
    "            T.add_edges_from(true_comb)\n",
    "            true_components = list(nx.connected_components(T))\n",
    "            set_true = {frozenset(sublist) for sublist in true_components}\n",
    "            set_pred = {frozenset(sublist) for sublist in connected_components}\n",
    "            evt_type =  check_matrix(set_true,set_pred)\n",
    "            if (evt_type == sel_type):\n",
    "                for k in range(len(connected_components)):\n",
    "                    cluster_data = evt[list(connected_components[k]),:]\n",
    "                    cluster_data_cart = sph2cart(cluster_data)\n",
    "                    cluster_data_cart_cms = func_cm(cluster_data_cart)\n",
    "                    cluster_data_sph_cms = cart2sph(cluster_data_cart_cms)\n",
    "                    cluster_data_sph_cms[:,4] = cluster_data_sph_cms[:,4] -4500\n",
    "                    arr = np.vstack((arr,cluster_data_sph_cms[0,:].reshape(-1,5)))\n",
    "            idx += len(pair_indices)\n",
    "        unique_values, continuous_numbers = np.unique(arr[:,0], return_inverse=True)\n",
    "        arr[:,0] = continuous_numbers\n",
    "        np.savetxt(f,arr.reshape(-1,5),fmt = fmt, delimiter=\",\")\n",
    "    f.close()\n",
    "    return f\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e451329-6771-40c7-b286-85c362bfb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read all lines and remove leading/trailing whitespace\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Use a set to eliminate duplicates while preserving order\n",
    "    unique_lines = list(dict.fromkeys(line.strip() for line in lines))\n",
    "    \n",
    "    # Write the unique lines back to the file or a new file\n",
    "    with open(file_path, 'w') as file:\n",
    "        for line in unique_lines:\n",
    "            file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf65cf7a-1004-44b6-a91b-4a9331a880ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n",
      "Progress: 0.00%\n",
      "Progress: 15.00%\n",
      "Progress: 30.00%\n",
      "Progress: 45.00%\n",
      "Progress: 60.00%\n",
      "Progress: 75.00%\n",
      "Progress: 90.00%\n",
      "now I have all simple stuff done\n",
      "Progress: 0.00%\n",
      "Progress: 15.00%\n",
      "Progress: 30.00%\n",
      "Progress: 45.00%\n",
      "Progress: 60.00%\n",
      "Progress: 75.00%\n",
      "Progress: 90.00%\n"
     ]
    }
   ],
   "source": [
    "#data = genfromtxt('raw_data_test.txt', delimiter=',')\n",
    "#data = genfromtxt('final_uniform_validation.txt', delimiter=',')\n",
    "data = genfromtxt('./sim_data/data_uniform/data_raw_uniform_training.csv', delimiter=',')\n",
    "#data = genfromtxt('validation_raw.txt',delimiter=',')\n",
    "#data = genfromtxt('data_stream_2121.txt', delimiter=',')\n",
    "data[:,4] = data[:,4]+4500  #this step is needed, I only want positive time values, so that I can use the time as a radius\n",
    "#my_data[:,4] = 1  #this step is needed, I only want positive time values, so that I can use the time as a radius\n",
    "\n",
    "# ### structure of mydata : eventnr, energy, theta, phi, hit-time\n",
    "data = data*[1.,1.,3.14159/180,3.14159/180,1.]\n",
    "unique_values, continuous_numbers = np.unique(data[:,0], return_inverse=True)\n",
    "data[:,0] = continuous_numbers\n",
    "test_reco_v = subset_data_sel(data,\"fn\",\"agglo\",\"false_neg_uniform.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0757f6-b1c4-4678-b62d-a1d8983049d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248c9861-f3cd-41d7-9fed-82d998115d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_duplicates_from_file('false_neg_uniform.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38798d6-00f9-49df-8de5-52a2f8fcbca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d55b7e2e-3991-4081-a912-f4b28de7effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1245285, 5)\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 5.11988e+05 5.11989e+05\n",
      " 5.11990e+05]\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 2.852144841333333 2.852144841333333 0.4149220085944444 0.4149220085944444\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 4.170949416777778 4.170949416777778 1.2820706617055555 1.2820706617055555\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 3.4802708552777775 3.4802708552777775 1.4284530477555553 1.4284530477555553\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 3.6742640377777778 3.6742640377777778 1.1901372663388887 1.1901372663388887\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 3.6742640377777778 3.6742640377777778 1.1901372663388887 1.1901372663388887\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 0.7276498398166665 0.7276498398166665 0.8523866707666666 0.8523866707666666\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 3.4802708552777775 3.4802708552777775 1.4284530477555553 1.4284530477555553\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 0.5326722924499999 0.5326722924499999 1.1901372663388887 1.1901372663388887\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 2.2168978900555554 2.2168978900555554 1.3316519332166665 1.3316519332166665\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 0.7276498398166665 0.7276498398166665 0.8523866707666666 0.8523866707666666\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 5.162819192888888 5.162819192888888 1.331397115361111 1.331397115361111\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 5.3588893154444435 5.3588893154444435 1.3774772593499998 1.3774772593499998\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 0.5326722924499999 0.5326722924499999 1.1901372663388887 1.1901372663388887\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 0.33867910995 0.33867910995 1.4284530477555553 1.4284530477555553\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 0.44253134871111105 0.44253134871111105 0.9321446595555555 0.9321446595555555\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 3.6742640377777778 3.6742640377777778 1.1901372663388887 1.1901372663388887\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 5.358487890055555 5.358487890055555 1.3316519332166665 1.3316519332166665\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 1.3159439832166666 1.3159439832166666 1.4744755959277778 1.4744755959277778\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 5.162819192888888 5.162819192888888 1.331397115361111 1.331397115361111\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 2.852144841333333 2.852144841333333 0.4149220085944444 0.4149220085944444\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 2.021229192888889 2.021229192888889 1.331397115361111 1.331397115361111\n",
      "1.0000000000000002\n",
      "something goes completely wrong!!!\n",
      "angles phi1,2, theta1,2: 3.4802708552777775 3.4802708552777775 1.4284530477555553 1.4284530477555553\n",
      "1.0000000000000002\n",
      "now I have all simple stuff done\n",
      "Progress: 0.00%\n",
      "Progress: 0.59%\n",
      "Progress: 1.17%\n",
      "Progress: 1.76%\n",
      "Progress: 2.34%\n",
      "Progress: 2.93%\n",
      "Progress: 3.52%\n",
      "Progress: 4.10%\n",
      "Progress: 4.69%\n",
      "Progress: 5.27%\n",
      "Progress: 5.86%\n",
      "Progress: 6.45%\n",
      "Progress: 7.03%\n",
      "Progress: 7.62%\n",
      "Progress: 8.20%\n",
      "Progress: 8.79%\n",
      "Progress: 9.38%\n",
      "Progress: 9.96%\n",
      "Progress: 10.55%\n",
      "Progress: 11.13%\n",
      "Progress: 11.72%\n",
      "Progress: 12.30%\n",
      "Progress: 12.89%\n",
      "Progress: 13.48%\n",
      "Progress: 14.06%\n",
      "Progress: 14.65%\n",
      "Progress: 15.23%\n",
      "Progress: 15.82%\n",
      "Progress: 16.41%\n",
      "Progress: 16.99%\n",
      "Progress: 17.58%\n",
      "Progress: 18.16%\n",
      "Progress: 18.75%\n",
      "Progress: 19.34%\n",
      "Progress: 19.92%\n",
      "Progress: 20.51%\n",
      "Progress: 21.09%\n",
      "Progress: 21.68%\n",
      "Progress: 22.27%\n",
      "Progress: 22.85%\n",
      "Progress: 23.44%\n",
      "Progress: 24.02%\n",
      "Progress: 24.61%\n",
      "Progress: 25.20%\n",
      "Progress: 25.78%\n",
      "Progress: 26.37%\n",
      "Progress: 26.95%\n",
      "Progress: 27.54%\n",
      "Progress: 28.13%\n",
      "Progress: 28.71%\n",
      "Progress: 29.30%\n",
      "Progress: 29.88%\n",
      "Progress: 30.47%\n",
      "Progress: 31.06%\n",
      "Progress: 31.64%\n",
      "Progress: 32.23%\n",
      "Progress: 32.81%\n",
      "Progress: 33.40%\n",
      "Progress: 33.98%\n",
      "Progress: 34.57%\n",
      "Progress: 35.16%\n",
      "Progress: 35.74%\n",
      "Progress: 36.33%\n",
      "Progress: 36.91%\n",
      "Progress: 37.50%\n",
      "Progress: 38.09%\n",
      "Progress: 38.67%\n",
      "Progress: 39.26%\n",
      "Progress: 39.84%\n",
      "Progress: 40.43%\n",
      "Progress: 41.02%\n",
      "Progress: 41.60%\n",
      "Progress: 42.19%\n",
      "Progress: 42.77%\n",
      "Progress: 43.36%\n",
      "Progress: 43.95%\n",
      "Progress: 44.53%\n",
      "Progress: 45.12%\n",
      "Progress: 45.70%\n",
      "Progress: 46.29%\n",
      "Progress: 46.88%\n",
      "Progress: 47.46%\n",
      "Progress: 48.05%\n",
      "Progress: 48.63%\n",
      "Progress: 49.22%\n",
      "Progress: 49.81%\n",
      "Progress: 50.39%\n",
      "Progress: 50.98%\n",
      "Progress: 51.56%\n",
      "Progress: 52.15%\n",
      "Progress: 52.74%\n",
      "Progress: 53.32%\n",
      "Progress: 53.91%\n",
      "Progress: 54.49%\n",
      "Progress: 55.08%\n",
      "Progress: 55.67%\n",
      "Progress: 56.25%\n",
      "Progress: 56.84%\n",
      "Progress: 57.42%\n",
      "Progress: 58.01%\n",
      "Progress: 58.59%\n",
      "Progress: 59.18%\n",
      "Progress: 59.77%\n",
      "Progress: 60.35%\n",
      "Progress: 60.94%\n",
      "Progress: 61.52%\n",
      "Progress: 62.11%\n",
      "Progress: 62.70%\n",
      "Progress: 63.28%\n",
      "Progress: 63.87%\n",
      "Progress: 64.45%\n",
      "Progress: 65.04%\n",
      "Progress: 65.63%\n",
      "Progress: 66.21%\n",
      "Progress: 66.80%\n",
      "Progress: 67.38%\n",
      "Progress: 67.97%\n",
      "Progress: 68.56%\n",
      "Progress: 69.14%\n",
      "Progress: 69.73%\n",
      "Progress: 70.31%\n",
      "Progress: 70.90%\n",
      "Progress: 71.49%\n",
      "Progress: 72.07%\n",
      "Progress: 72.66%\n",
      "Progress: 73.24%\n",
      "Progress: 73.83%\n",
      "Progress: 74.42%\n",
      "Progress: 75.00%\n",
      "Progress: 75.59%\n",
      "Progress: 76.17%\n",
      "Progress: 76.76%\n",
      "Progress: 77.35%\n",
      "Progress: 77.93%\n",
      "Progress: 78.52%\n",
      "Progress: 79.10%\n",
      "Progress: 79.69%\n",
      "Progress: 80.27%\n",
      "Progress: 80.86%\n",
      "Progress: 81.45%\n",
      "Progress: 82.03%\n",
      "Progress: 82.62%\n",
      "Progress: 83.20%\n",
      "Progress: 83.79%\n",
      "Progress: 84.38%\n",
      "Progress: 84.96%\n",
      "Progress: 85.55%\n",
      "Progress: 86.13%\n",
      "Progress: 86.72%\n",
      "Progress: 87.31%\n",
      "Progress: 87.89%\n",
      "Progress: 88.48%\n",
      "Progress: 89.06%\n",
      "Progress: 89.65%\n",
      "Progress: 90.24%\n",
      "Progress: 90.82%\n",
      "Progress: 91.41%\n",
      "Progress: 91.99%\n",
      "Progress: 92.58%\n",
      "Progress: 93.17%\n",
      "Progress: 93.75%\n",
      "Progress: 94.34%\n",
      "Progress: 94.92%\n",
      "Progress: 95.51%\n",
      "Progress: 96.10%\n",
      "Progress: 96.68%\n",
      "Progress: 97.27%\n",
      "Progress: 97.85%\n",
      "Progress: 98.44%\n",
      "Progress: 99.03%\n",
      "Progress: 99.61%\n"
     ]
    }
   ],
   "source": [
    "#data = genfromtxt('raw_data_test.txt', delimiter=',')\n",
    "#data = genfromtxt('final_uniform_validation.txt', delimiter=',')\n",
    "data = genfromtxt('./sim_data/data_final_uniform/final_uniform_validation.txt', delimiter=',')\n",
    "#data = genfromtxt('validation_raw.txt',delimiter=',')\n",
    "#data = genfromtxt('data_stream_2121.txt', delimiter=',')\n",
    "data[:,4] = data[:,4]+4500  #this step is needed, I only want positive time values, so that I can use the time as a radius\n",
    "#my_data[:,4] = 1  #this step is needed, I only want positive time values, so that I can use the time as a radius\n",
    "\n",
    "# ### structure of mydata : eventnr, energy, theta, phi, hit-time\n",
    "data = data*[1.,1.,3.14159/180,3.14159/180,1.]\n",
    "unique_values, continuous_numbers = np.unique(data[:,0], return_inverse=True)\n",
    "data[:,0] = continuous_numbers\n",
    "test_reco_v = subset_data_sel(data,\"fn\",\"r3b\",\"false_neg_uniform_r3b.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd425705-7810-40f9-aff9-cbacc5a81b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
