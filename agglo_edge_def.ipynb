{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a65f9b-5241-45a9-8241-69b8b6f5170b",
   "metadata": {},
   "source": [
    "### we simply need to compare output of edge, and agglo clustering\n",
    "### and where agglo clustering is 1 we leave to 1 and do not take the \n",
    "### predictions from edge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7c02c5-62e1-401d-9885-1521e4409bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from itertools import combinations\n",
    "import torch\n",
    "import math \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%run r3b_clustering_def.ipynb\n",
    "%run agglomerative_def.ipynb\n",
    "%run ff_models_def.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f74032-4780-49b7-8d92-6e19d5307ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agglo_edge_ene(data,agglo_vals,edge_vals,true_vals):\n",
    "    ene_list = []\n",
    "    ene_true_list = []\n",
    "    idx = 0\n",
    "    well_reco = 0\n",
    "    array_unique_events = np.unique(data[:,0])\n",
    "    merge_tries = 0\n",
    "    correctly_merged = 0\n",
    "    for i in range(0,int(np.max(data,axis=0)[0])-(int(np.max(data,axis=0)[0])%3) ,3):\n",
    "        mask = ((data[:,0] == i) | (data[:,0] == i+1) | (data[:,0] == i+2))\n",
    "        evt = data[mask]\n",
    "        pair_indices = list(combinations(range(evt.shape[0]), 2))\n",
    "        ##pair indices which belong together...\n",
    "        pred_comb = []\n",
    "        ##TJ this is the new part with true stuff\n",
    "        true_comb = []\n",
    "        for m in range(evt.shape[0]):\n",
    "            pred_comb.append((m,m))\n",
    "            true_comb.append((m,m))\n",
    "        for j in range (len(pair_indices)):\n",
    "            if agglo_vals[idx+j] > 0.75:\n",
    "                pred_comb.append(pair_indices[j])\n",
    "            if ((agglo_vals[idx+j] < 0.75) and (edge_vals[idx+j] > 0.75)):\n",
    "                merge_tries +=1\n",
    "                pred_comb.append(pair_indices[j])\n",
    "                if true_vals[idx+j] > 0.75: \n",
    "                    correctly_merged +=1\n",
    "            if true_vals[idx+j] > 0.75:   \n",
    "                true_comb.append(pair_indices[j])\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(pred_comb)\n",
    "        connected_components = list(nx.connected_components(G))\n",
    "        ##TJ again, this it the addition for the true values\n",
    "        T = nx.Graph()\n",
    "        T.add_edges_from(true_comb)\n",
    "        true_components = list(nx.connected_components(T))\n",
    "        set_true = {frozenset(sublist) for sublist in true_components}\n",
    "        set_pred = {frozenset(sublist) for sublist in connected_components}\n",
    "\n",
    "        # Find the intersection of the two sets\n",
    "        common_sublists = set_true.intersection(set_pred)\n",
    "\n",
    "        well_reco += len(common_sublists)\n",
    "        for k in range(len(connected_components)):\n",
    "            columns_to_sum = evt[list(connected_components[k]),1]\n",
    "            ene = np.sum(columns_to_sum)\n",
    "            ene_list.append(ene)\n",
    "        #make list true energy\n",
    "        for l in range(len(true_components)):\n",
    "            true_columns_to_sum = evt[list(true_components[l]),1]\n",
    "            ene_true = np.sum(true_columns_to_sum)\n",
    "            ene_true_list.append(ene_true)\n",
    "        idx += len(pair_indices)\n",
    "    single_hit_energies = []\n",
    "    for i in range(data.shape[0]):\n",
    "        single_hit_energies.append(data[i,1])\n",
    "        \n",
    "    #plt.hist(single_hit_energies,bins=70,range=(0,7),label=\"single hit energy\",color=\"green\",alpha=0.5)    \n",
    "    plt.hist(ene_list,bins=100,range=(0,10),label=\"reconstructed energy\",color=\"red\",alpha=0.3)\n",
    "    plt.grid()\n",
    "    plt.hist(ene_true_list,bins=100,range=(0,10),label=\"true energies\",color=\"blue\",alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    well_reco = float(well_reco)/float(np.max(data,axis=0)[0])\n",
    "    print(\"percentage of correctly merged hits/clusters:\")\n",
    "    print(correctly_merged/merge_tries)\n",
    "\n",
    "    print(\"well reco...\", well_reco)\n",
    "    return (ene_list,ene_true_list,well_reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdcc79-8d3b-4dc0-ae09-d6e20aa8ea4b",
   "metadata": {},
   "source": [
    "### 1)first run feed forward model on false negative to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3313ab-2c9a-4c5a-9f9c-7eac56bd5b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46472/2638893087.py:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  input_data = torch.FloatTensor(ll)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear.weight \t torch.Size([1000, 12])\n",
      "linear.bias \t torch.Size([1000])\n",
      "another_linear.weight \t torch.Size([100, 1000])\n",
      "another_linear.bias \t torch.Size([100])\n",
      "another_linear_two.weight \t torch.Size([100, 100])\n",
      "another_linear_two.bias \t torch.Size([100])\n",
      "linear_back.weight \t torch.Size([1, 100])\n",
      "linear_back.bias \t torch.Size([1])\n",
      "epochnr: 0 0.0 %\n",
      "epochnr: 1 0.01 %\n",
      "epochnr: 2 0.02 %\n",
      "epochnr: 3 0.03 %\n",
      "epochnr: 4 0.04 %\n",
      "epochnr: 5 0.05 %\n",
      "epochnr: 6 0.06 %\n",
      "epochnr: 7 0.06999999999999999 %\n",
      "epochnr: 8 0.08 %\n",
      "epochnr: 9 0.09 %\n",
      "epochnr: 10 0.1 %\n",
      "epochnr: 11 0.11 %\n",
      "epochnr: 12 0.12 %\n",
      "epochnr: 13 0.13 %\n",
      "epochnr: 14 0.13999999999999999 %\n",
      "epochnr: 15 0.15 %\n",
      "epochnr: 16 0.16 %\n",
      "epochnr: 17 0.16999999999999998 %\n",
      "epochnr: 18 0.18 %\n",
      "epochnr: 19 0.19 %\n",
      "epochnr: 20 0.2 %\n",
      "epochnr: 21 0.21 %\n",
      "epochnr: 22 0.22 %\n",
      "epochnr: 23 0.22999999999999998 %\n",
      "epochnr: 24 0.24 %\n",
      "epochnr: 25 0.25 %\n",
      "epochnr: 26 0.26 %\n",
      "epochnr: 27 0.27 %\n",
      "epochnr: 28 0.27999999999999997 %\n",
      "epochnr: 29 0.29 %\n",
      "epochnr: 30 0.3 %\n",
      "epochnr: 31 0.31 %\n",
      "epochnr: 32 0.32 %\n",
      "epochnr: 33 0.33 %\n",
      "epochnr: 34 0.33999999999999997 %\n",
      "epochnr: 35 0.35000000000000003 %\n",
      "epochnr: 36 0.36 %\n",
      "epochnr: 37 0.37 %\n",
      "epochnr: 38 0.38 %\n",
      "epochnr: 39 0.38999999999999996 %\n",
      "epochnr: 40 0.4 %\n",
      "epochnr: 41 0.41000000000000003 %\n",
      "epochnr: 42 0.42 %\n",
      "epochnr: 43 0.43 %\n",
      "epochnr: 44 0.44 %\n",
      "epochnr: 45 0.44999999999999996 %\n",
      "epochnr: 46 0.45999999999999996 %\n",
      "epochnr: 47 0.47000000000000003 %\n",
      "epochnr: 48 0.48 %\n",
      "epochnr: 49 0.49 %\n",
      "epochnr: 50 0.5 %\n",
      "epochnr: 51 0.51 %\n",
      "epochnr: 52 0.52 %\n",
      "epochnr: 53 0.53 %\n",
      "epochnr: 54 0.54 %\n",
      "epochnr: 55 0.5499999999999999 %\n",
      "epochnr: 56 0.5599999999999999 %\n",
      "epochnr: 57 0.5700000000000001 %\n",
      "epochnr: 58 0.58 %\n",
      "epochnr: 59 0.59 %\n",
      "epochnr: 60 0.6 %\n",
      "epochnr: 61 0.61 %\n",
      "epochnr: 62 0.62 %\n",
      "epochnr: 63 0.63 %\n",
      "epochnr: 64 0.64 %\n",
      "epochnr: 65 0.65 %\n",
      "epochnr: 66 0.66 %\n",
      "epochnr: 67 0.67 %\n",
      "epochnr: 68 0.6799999999999999 %\n",
      "epochnr: 69 0.69 %\n",
      "epochnr: 70 0.7000000000000001 %\n",
      "epochnr: 71 0.7100000000000001 %\n",
      "epochnr: 72 0.72 %\n",
      "epochnr: 73 0.73 %\n",
      "epochnr: 74 0.74 %\n",
      "epochnr: 75 0.75 %\n",
      "epochnr: 76 0.76 %\n",
      "epochnr: 77 0.77 %\n",
      "epochnr: 78 0.7799999999999999 %\n",
      "epochnr: 79 0.79 %\n",
      "epochnr: 80 0.8 %\n",
      "epochnr: 81 0.8099999999999999 %\n",
      "epochnr: 82 0.8200000000000001 %\n",
      "epochnr: 83 0.83 %\n",
      "epochnr: 84 0.84 %\n",
      "epochnr: 85 0.8500000000000001 %\n",
      "epochnr: 86 0.86 %\n",
      "epochnr: 87 0.8699999999999999 %\n",
      "epochnr: 88 0.88 %\n",
      "epochnr: 89 0.89 %\n",
      "epochnr: 90 0.8999999999999999 %\n",
      "epochnr: 91 0.91 %\n",
      "epochnr: 92 0.9199999999999999 %\n",
      "epochnr: 93 0.9299999999999999 %\n",
      "epochnr: 94 0.9400000000000001 %\n",
      "epochnr: 95 0.95 %\n",
      "epochnr: 96 0.96 %\n",
      "epochnr: 97 0.97 %\n",
      "epochnr: 98 0.98 %\n",
      "epochnr: 99 0.9900000000000001 %\n",
      "epochnr: 100 1.0 %\n",
      "epochnr: 101 1.01 %\n",
      "epochnr: 102 1.02 %\n",
      "epochnr: 103 1.03 %\n",
      "epochnr: 104 1.04 %\n",
      "epochnr: 105 1.05 %\n",
      "epochnr: 106 1.06 %\n",
      "epochnr: 107 1.0699999999999998 %\n",
      "epochnr: 108 1.08 %\n",
      "epochnr: 109 1.09 %\n",
      "epochnr: 110 1.0999999999999999 %\n",
      "epochnr: 111 1.11 %\n",
      "epochnr: 112 1.1199999999999999 %\n",
      "epochnr: 113 1.13 %\n",
      "epochnr: 114 1.1400000000000001 %\n",
      "epochnr: 115 1.15 %\n",
      "epochnr: 116 1.16 %\n",
      "epochnr: 117 1.17 %\n",
      "epochnr: 118 1.18 %\n",
      "epochnr: 119 1.1900000000000002 %\n",
      "epochnr: 120 1.2 %\n",
      "epochnr: 121 1.21 %\n",
      "epochnr: 122 1.22 %\n",
      "epochnr: 123 1.23 %\n",
      "epochnr: 124 1.24 %\n",
      "epochnr: 125 1.25 %\n",
      "epochnr: 126 1.26 %\n",
      "epochnr: 127 1.27 %\n",
      "epochnr: 128 1.28 %\n",
      "epochnr: 129 1.29 %\n",
      "epochnr: 130 1.3 %\n",
      "epochnr: 131 1.31 %\n",
      "epochnr: 132 1.32 %\n",
      "epochnr: 133 1.3299999999999998 %\n",
      "epochnr: 134 1.34 %\n",
      "epochnr: 135 1.35 %\n",
      "epochnr: 136 1.3599999999999999 %\n",
      "epochnr: 137 1.37 %\n",
      "epochnr: 138 1.38 %\n",
      "epochnr: 139 1.39 %\n",
      "epochnr: 140 1.4000000000000001 %\n",
      "epochnr: 141 1.41 %\n",
      "epochnr: 142 1.4200000000000002 %\n",
      "epochnr: 143 1.43 %\n",
      "epochnr: 144 1.44 %\n",
      "epochnr: 145 1.4500000000000002 %\n",
      "epochnr: 146 1.46 %\n",
      "epochnr: 147 1.47 %\n",
      "epochnr: 148 1.48 %\n",
      "epochnr: 149 1.49 %\n",
      "epochnr: 150 1.5 %\n",
      "epochnr: 151 1.51 %\n",
      "epochnr: 152 1.52 %\n",
      "epochnr: 153 1.53 %\n",
      "epochnr: 154 1.54 %\n",
      "epochnr: 155 1.55 %\n",
      "epochnr: 156 1.5599999999999998 %\n",
      "epochnr: 157 1.5699999999999998 %\n",
      "epochnr: 158 1.58 %\n",
      "epochnr: 159 1.59 %\n",
      "epochnr: 160 1.6 %\n",
      "epochnr: 161 1.6099999999999999 %\n",
      "epochnr: 162 1.6199999999999999 %\n",
      "epochnr: 163 1.63 %\n",
      "epochnr: 164 1.6400000000000001 %\n",
      "epochnr: 165 1.6500000000000001 %\n",
      "epochnr: 166 1.66 %\n",
      "epochnr: 167 1.67 %\n",
      "epochnr: 168 1.68 %\n",
      "epochnr: 169 1.69 %\n",
      "epochnr: 170 1.7000000000000002 %\n",
      "epochnr: 171 1.71 %\n",
      "epochnr: 172 1.72 %\n",
      "epochnr: 173 1.73 %\n",
      "epochnr: 174 1.7399999999999998 %\n",
      "epochnr: 175 1.7500000000000002 %\n",
      "epochnr: 176 1.76 %\n",
      "epochnr: 177 1.77 %\n",
      "epochnr: 178 1.78 %\n",
      "epochnr: 179 1.79 %\n",
      "epochnr: 180 1.7999999999999998 %\n",
      "epochnr: 181 1.81 %\n",
      "epochnr: 182 1.82 %\n",
      "epochnr: 183 1.83 %\n",
      "epochnr: 184 1.8399999999999999 %\n",
      "epochnr: 185 1.8499999999999999 %\n",
      "epochnr: 186 1.8599999999999999 %\n",
      "epochnr: 187 1.87 %\n",
      "epochnr: 188 1.8800000000000001 %\n",
      "epochnr: 189 1.8900000000000001 %\n",
      "epochnr: 190 1.9 %\n",
      "epochnr: 191 1.91 %\n",
      "epochnr: 192 1.92 %\n",
      "epochnr: 193 1.9300000000000002 %\n",
      "epochnr: 194 1.94 %\n",
      "epochnr: 195 1.95 %\n",
      "epochnr: 196 1.96 %\n",
      "epochnr: 197 1.97 %\n",
      "epochnr: 198 1.9800000000000002 %\n",
      "epochnr: 199 1.9900000000000002 %\n",
      "epochnr: 200 2.0 %\n",
      "epochnr: 201 2.01 %\n",
      "epochnr: 202 2.02 %\n",
      "epochnr: 203 2.03 %\n",
      "epochnr: 204 2.04 %\n",
      "epochnr: 205 2.0500000000000003 %\n",
      "epochnr: 206 2.06 %\n",
      "epochnr: 207 2.07 %\n",
      "epochnr: 208 2.08 %\n",
      "epochnr: 209 2.09 %\n",
      "epochnr: 210 2.1 %\n",
      "epochnr: 211 2.11 %\n",
      "epochnr: 212 2.12 %\n",
      "epochnr: 213 2.13 %\n",
      "epochnr: 214 2.1399999999999997 %\n",
      "epochnr: 215 2.15 %\n",
      "epochnr: 216 2.16 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m training_true \u001b[38;5;241m=\u001b[39m get_true_combinatorics(training_data)\n\u001b[1;32m      9\u001b[0m input_training_data \u001b[38;5;241m=\u001b[39m manip_data_ff(training_data,\u001b[38;5;241m12\u001b[39m) \u001b[38;5;66;03m##second param gives number of features\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pred_training \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_two_hidden_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_training_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m training_reco \u001b[38;5;241m=\u001b[39m get_true_reco_ene(training_data,pred_training[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),training_true)\n",
      "File \u001b[0;32m/tmp/ipykernel_46472/3521851979.py:20\u001b[0m, in \u001b[0;36mtrain_two_hidden_model\u001b[0;34m(input_data, true_data, in_features, features, learning_rate, hidden_features)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss_val\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#print(f'Finished epoch {epoch}, latest loss {loss}')\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m##TJ just uncomment this line for now...\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#plt.plot(loss_val)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data = genfromtxt('training_all_output_false_negative.txt', delimiter=',')\n",
    "training_data[:,4] =(training_data[:,4]-np.min(training_data[:,4]))/(np.max(training_data[:,4])-np.min(training_data[:,4]))\n",
    "unique_values, continuous_numbers = np.unique(training_data[:,0], return_inverse=True)\n",
    "training_data[:,0] = continuous_numbers\n",
    "training_data[:,2] = training_data[:,2]*math.pi/180.\n",
    "training_data[:,3] = training_data[:,3]*math.pi/180.\n",
    "training_true = get_true_combinatorics(training_data)\n",
    "\n",
    "input_training_data = manip_data_ff(training_data,12) ##second param gives number of features\n",
    "\n",
    "pred_training = train_two_hidden_model(input_training_data,torch.FloatTensor(training_true),12,1000,5e-3,100)\n",
    "\n",
    "training_reco = get_true_reco_ene(training_data,pred_training[0].tolist(),training_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2088c2-13b3-4f6d-8f2c-7e56380dc02c",
   "metadata": {},
   "source": [
    "### 2) then run the agglo model on the raw dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135750ae-89a1-49e7-8376-da7be7464c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = genfromtxt('validation_raw.txt', delimiter=',')\n",
    "data[:,4] = data[:,4]+4500  #this step is needed, I only want positive time values, so that I can use the time as a radius\n",
    "\n",
    "# ### structure of mydata : eventnr, energy, theta, phi, hit-time\n",
    "data = data*[1.,1.,3.14159/180,3.14159/180,1.]\n",
    "unique_values, continuous_numbers = np.unique(data[:,0], return_inverse=True)\n",
    "data[:,0] = continuous_numbers\n",
    "out_cluster_agglo = run_agglo_clustering(data,3540)\n",
    "\n",
    "agglo_true = get_true_combinatorics(data)\n",
    "agglo_reco = get_true_reco_ene(data,out_cluster_agglo,agglo_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8bf8a-08d2-47cd-99de-6a86d02e6e45",
   "metadata": {},
   "source": [
    "### 3) then run the pretrained ff model on the raw dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61751902-af6e-4c39-b29f-333e4ce349b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = genfromtxt('validation_raw.txt', delimiter=',')\n",
    "## some data manipulation needed\n",
    "val_data[:,4] =(val_data[:,4]-np.min(val_data[:,4]))/(np.max(val_data[:,4])-np.min(val_data[:,4]))\n",
    "unique_values, continuous_numbers = np.unique(val_data[:,0], return_inverse=True)\n",
    "val_data[:,0] = continuous_numbers\n",
    "val_data[:,2] = val_data[:,2]*math.pi/180.\n",
    "val_data[:,3] = val_data[:,3]*math.pi/180.\n",
    "val_true = get_true_combinatorics(val_data)\n",
    "input_val_data = manip_data_ff(val_data,12) ##second param gives number of features\n",
    "\n",
    "pred_val = validate_two_hidden_model(input_val_data,torch.FloatTensor(val_true),12,1000,5e-3,100)\n",
    "\n",
    "val_reco = get_true_reco_ene(val_data,pred_val[0].tolist(),val_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20648a25-d8e7-4a92-ad3a-fba3cc81586a",
   "metadata": {},
   "source": [
    "### 4) combine the outputs of the agglo model and of the ff model and insert them into the \"get_agglo_edge_ene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7c6fe-e390-414c-bb18-9e76c84a4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo_edge_val = get_agglo_edge_ene(val_data,out_cluster_agglo,pred_val[0].tolist(),val_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
